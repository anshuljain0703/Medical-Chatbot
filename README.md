# ğŸ©º Medical AI-powered Question Answering Chatbot  

An **AI-powered Question Answering (QA) Chatbot** designed to provide **reliable medical information** using **Retrieval-Augmented Generation (RAG)**. Unlike traditional LLMs that may hallucinate, this chatbot grounds its responses in trusted sources like the *Gale Encyclopedia of Medicine*.  

---

## ğŸš€ Features  
- **LangChain** â€“ conversational workflow & retrieval pipeline  
- **Hugging Face Embeddings** â€“ semantic query understanding  
- **Mistral 7B** â€“ LLM backend for high-quality natural language answers  
- **FAISS** â€“ vector database for efficient similarity search  
- **Groq Cloud API** â€“ enabling fast and scalable inference  
- **Streamlit UI** â€“ user-friendly chatbot interface  
- **RAG Pipeline** â€“ ensures context-aware and fact-grounded answers  

---

## ğŸ› ï¸ Tech Stack  
- Python 3.9+  
- [LangChain](https://www.langchain.com/)  
- [Hugging Face](https://huggingface.co/)  
- [Mistral 7B](https://mistral.ai/)  
- [FAISS](https://faiss.ai/)  
- [Groq Cloud](https://groq.com/)  
- [Streamlit](https://streamlit.io/)  

---

## âš™ï¸ How It Works  
1. ğŸ“„ Load medical PDFs (e.g., Gale Encyclopedia of Medicine)  
2. âœ‚ï¸ Split documents into chunks using LangChain text splitters  
3. ğŸ” Generate embeddings with Hugging Face models  
4. ğŸ“¦ Store embeddings in FAISS for similarity search  
5. ğŸ”— Retrieve relevant context when a user asks a question  
6. ğŸ§  Mistral LLM generates answers based only on retrieved context  
7. ğŸ’¬ Streamlit chatbot UI delivers precise, reliable responses  


---

## ğŸ”‘ Environment Variables  
Create a `.env` file in the root directory with:  

```ini
HF_TOKEN=your_huggingface_api_key
GROQ_API_KEY=your_groq_api_key




